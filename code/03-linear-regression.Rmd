---
title: "Communities and Crime"
subtitle: "Statistical Learning Final Exam Project"
chapter: "Multiple Linear Regression"
author: "Caria Natascia, Cozzolino Claudia, Petrella Alfredo"
date: "June 20, 2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, include=FALSE}
# set working directory
# setwd("Project/code")
```

```{r, include=FALSE}
Col = c("#EF476F", "#FFD166", "#06D6A0", "#118AB2", "#073B4C")
Rgb = c(rgb(0.94, 0.28, 0.44, 0.7),
        rgb(1, 0.82, 0.4, 0.7),
        rgb(0.02, 0.84, 0.63, 0.7),
        rgb(0.07, 0.54, 0.7, 0.7),
        rgb(0.03, 0.23, 0.3, 0.7))
```

# Multiple Linear Regression

For almost the entire project, we decided to focus the analyzes on the ViolentCrimePerPop variable only, i.e. the rate of the sum of the violent crimes, according to the US laws, per population.
In particular our main task is to predict this response from the socio-economic and demographic attributes per community provided by the dataset using a multiple linear regression model.

For this purpose, we firstly remove non predictive variables, such as the community name and all the other crimes columns except for the ViolentCrimesPerPop response. Note that in this first study we also discarded the categorical attribute state, assuming it as not influential.

```{r}
# import cleaned and standardized dataset
cleandf <- read.csv("../data/crimedata-cleaned.csv", row.names = 1)
standf <- read.csv("../data/crimedata-cleaned-stand.csv", row.names = 1)
```

```{r}
coltodrop <- c(1,2, seq(103,120)[-17]) # -17 keeps ViolentCrimesPerPop

df <- standf[,-coltodrop]
```

We then fit our multiple linear regression model and show statistics and plot about the residuals.
```{r}
reg.out <- lm(ViolentCrimesPerPop~., data=df)
```

```{r}
se  <- summary(reg.out)$sigma                  # se
rsq <- summary(reg.out)$r.squared              # R^2
adrsq <- summary(reg.out)$adj.r.squared        # adj R^2
cat("RSE:", round(se,2), "\n")
cat("R^2:", round(rsq,2), "\n")
cat("adjusted R^2:", round(adrsq,2))
```

```{r, fig.height=10, fig.width=16}
par(mfrow=c(2,2))
plot(reg.out)
par(mfrow=c(1,1))
```

To analyze in detail the results obtained from this first simple model, we recall the necessary theoretical assumptions that must be verified.

## Assessing Model Assumptions:
The model assumptions are:

- Linearity of the response-predictor relationships;
- Homeschedasticity of the error terms: Var($\epsilon_i$) = $\sigma^2$.

To check the conditions listed above we use the residual plots provided by lm.

### Linearity
The plot of residuals versus fitted values shows a little pattern, however it seems to indicate that there are linear associations in the data and that the errors are uncorrelated. 

### Homoschedasticity
The presence of a funnel shape in the residual plot suggests that the error terms do not have a constant variance. One possible
solution is to transform the response variable $Y$ using a concave function such as $\log(Y)$ or $\sqrt{Y}$.

With a trial and error approach we came out with the best transformation: $\log(Y)+1$, of which the resulting density is shown the following figures.

```{r, fig.height=4, fig.width=8}
par(mfrow=c(1,2))
hist(df$ViolentCrimesPerPop, prob=TRUE, breaks=30, col=Col[1], main="Violent Crimes",
     xlab = "ViolentCrimesPerPop")
lines(density(df$ViolentCrimesPerPop))

hist(log(df$ViolentCrimesPerPop +1), prob=TRUE, breaks=30, col=Col[1],
     main="log(Violent Crimes +1)", xlab = "log(ViolentCrimesPerPop +1)")
lines(density(log(df$ViolentCrimesPerPop +1)))
par(mfrow=c(1,1))
```

```{r, fig.height=4, fig.width=8}
par(mfrow=c(1,2))
qqnorm(df$ViolentCrimesPerPop, main="Violent Crimes")
qqline(df$ViolentCrimesPerPop)

qqnorm(log(df$ViolentCrimesPerPop +1), main="log(Violent Crimes +1)")
qqline(log(df$ViolentCrimesPerPop +1))
par(mfrow=c(1,1))
```

At this point we fitted again the linear model using the transformed response.

```{r}
reg.out2 <- lm(log(ViolentCrimesPerPop+1)~., data=df)
```

```{r}
se  <- summary(reg.out2)$sigma                  # se
rsq <- summary(reg.out2)$r.squared              # R^2
adrsq <- summary(reg.out2)$adj.r.squared        # adj R^2
cat("RSE:", round(se,2), "\n")
cat("R^2:", round(rsq,2), "\n")
cat("adjusted R^2:", round(adrsq,2))
```

```{r, fig.height=10, fig.width=16}
par(mfrow=c(2,2)) 
plot(reg.out2)
par(mfrow=c(1,1))
```

The plots of residuals versus fitted values shows that such a transformation leads to a reduction in heteroscedasticity, moreover it managed to considerably improve the statistic measures.


## Other Possible Problems

### Outliers
The residual plot identifies some outliers. However, it can be difficult to decide how large a residual needs to be before we consider the point to be an outlier. To address this problem, instead of plotting the residuals, we can plot the studentized residuals, computed
by dividing each residual $e_i$ by its estimated standard error. Observations whose studentized residuals are greater than 3 in
absolute value are possible outliers.

Note that the empirical motivation for the value equal to 3 is that the Studentized Residuals are approximated by a $N(0,1)$. The probability to observe a value greater than 3 is then 0.001349898[1].

```{r}
1 - pnorm(3)
```

```{r, fig.height=4, fig.width=6}
plot(predict(reg.out2), rstandard(reg.out2), xlab="Fitted Values", ylab = "Studentized Residuals")

abline(h=3, col = "red")
abline(h=-3, col = "red")
```

```{r}
out <- names(rstandard(reg.out2)[(abs(rstandard(reg.out2)) > 3)])

standf[out,c(1,2)]

cityout<-standf$communityname[rownames(standf) %in% out]
```

```{r}
citycord <- read.csv("../data/cities.csv")

# Rename column 
colnames(citycord)[colnames(citycord) == "city"] <- "communityname"
colnames(citycord)[colnames(citycord) == "latitude"] <- "lat"
colnames(citycord)[colnames(citycord) == "longitude"] <- "lon"


citycord<-merge(citycord,cleandf,by=c("state","communityname"))
```

```{r}
dfplot <- citycord[citycord$communityname %in% cityout, c("lon","lat","ViolentCrimesPerPop","communityname")]
```

Using this selection criterion, 20 communities are to be considered outliers. In a study like the one we are conducting, in which socio-economic, environmental and demographic information is fundamental, it might be interesting to investigate where these cities are located geographically. The following map shows the outliers communities with respect the ViolentCrimesPerPop variable. It can be noticed that the large majiority are clusterized in the state of NY and in the southern regions.

```{r, include=FALSE}
library(ggplot2)
library(usmap)
```

```{r, warning=FALSE}

cities_t <- usmap_transform(dfplot)

#without city labels
plot_usmap(fill = "#06D6A0", alpha = 0.25) +
  geom_point(data = cities_t,
             aes(x = lon.1, y = lat.1, size = ViolentCrimesPerPop),
             color = "#073B4C", alpha = 0.5) +
  scale_size_continuous(range = c(1, 16),
                        label = scales::comma) +
  labs(title = "Outliers communities",
       size = "Violent Crimes per 100 K inhabitants") +
  theme(legend.position = "right")

#with city labels
plot_usmap(fill = "#06D6A0", alpha = 0.25) +
  ggrepel::geom_label_repel(data = cities_t,
             aes(x = lon.1, y = lat.1, label = communityname),
             size = 2.5, alpha = 0.8,
             label.r = unit(0.5, "lines"), label.size = 0.55,
             segment.color = "black", segment.size = 0.7,
             seed = 1002) +
  geom_point(data = cities_t,
             aes(x = lon.1, y = lat.1, size = ViolentCrimesPerPop),
             color = "#073B4C", alpha = 0.5) +
  scale_size_continuous(range = c(1, 16),
                        label = scales::comma) +
  labs(title = "Outliers communities",
       size = "Violent Crimes per 100 K inhabitants") +
  theme(legend.position = "right")

```


The results of the linear regressor fitted without the outliers are shown below.

```{r}
# regression without outliers
reg.out3 <- lm(log(ViolentCrimesPerPop+1)~., data=df[!(rownames(df) %in% out),])
```

```{r}
se  <- summary(reg.out3)$sigma                  # se
rsq <- summary(reg.out3)$r.squared              # R^2
adrsq <- summary(reg.out3)$adj.r.squared        # adj R^2
cat("RSE:", round(se,2), "\n")
cat("R^2:", round(rsq,2), "\n")
cat("adjusted R^2:", round(adrsq,2))
```

```{r, fig.height=10, fig.width=16}
par(mfrow=c(2,2)) 
plot(reg.out3)
par(mfrow=c(1,1))
```

As expected, the RSE is smaller: 0.56 when the outliers are included versus 0.52 when they are removed. Nevertheless, since the RSE is used to compute all confidence intervals and p-values, this can have implications for the interpretation of the fit, as usual then, care should be taken in the decision of taking or dropping outliers.
For the purpose of this work we have decided not to remove them, not only to avoid the risk of losing statistical information, but because they seem to have a geographical meaning and dropping them could make the study less consistent.

### High Leverage Points
A second problem when dealing with regression is the presence of high leverage points. In order to quantify an observations leverage, we compute the leverage statistic. If a given observation has a leverage statistic that greatly exceeds $(p+1)/n$, then we may suspect that the corresponding point has high leverage.

```{r, fig.height=4, fig.width=6}
hv <- hatvalues(reg.out2)
plot(hv, rstandard(reg.out2), xlab="Leverage", ylab = "Studentized Residuals")

p <- dim(df)[2]-1
n <- dim(df)[1]
abline(v=(p+1)/n, col = "red")
```

The previous figure shows that a lot of points fall to the right of the critical value, in order not to lose too much data we will consider high leverage only the points above $3(p+1)/n$ [8](https://online.stat.psu.edu/stat462/node/171/). Using this new rule it appears that 56 communities are to be considered high leverage, also in this case we decided to investigate the corresponding geographic locations through the map. In this particular case, they are drawn according to the total population.

```{r}
lev <- names(hv[hv>3*(p+1)/n])
standf[lev,c(1,2)]

citylev<-standf$communityname[rownames(standf) %in% lev]
```

```{r}
dfplot<-citycord[citycord$communityname %in% citylev,c("lon","lat","communityname","state","population")]
```

```{r, warning=FALSE}

cities_t <- usmap_transform(dfplot)


#without city labels
plot_usmap(fill = "#f26b8b", alpha = 0.23) +
  geom_point(data = cities_t,
             aes(x = lon.1, y = lat.1, size = population),
             color = "#a7314d", alpha = 0.5) +
  scale_size_continuous(range = c(1, 16),
                        label = scales::comma) +
  labs(title = "Leverage communities",
       size = "Total community population") +
  theme(legend.position = "right")

#with city labels
plot_usmap(fill = "#f26b8b", alpha = 0.23) +
  ggrepel::geom_label_repel(data = cities_t,
             aes(x = lon.1, y = lat.1, label = communityname),
             size = 2, alpha = 0.8,
             label.r = unit(0.5, "lines"), label.size = 0.01,
             segment.color = "black", segment.size = 0.5,
             seed = 1002) +
  geom_point(data = cities_t,
             aes(x = lon.1, y = lat.1, size = population),
             color = "#a7314d", alpha = 0.5) +
  scale_size_continuous(range = c(1, 16),
                        label = scales::comma) +
  labs(title = "Leverage communities",
       size = "Total community population") +
  theme(legend.position = "right")

```

It is interesting to note that these cities belong in many cases to the most populous states in the US such as NY, CA, NJ, PA, TX, FL or to the the least inhabited such as AK and UT. Moreover, as for the outliers, the large majiority of them are clusterized in particular regions or states (CA, NY, NJ).


The results of the linear regressor fitted without the high leverage points are shown below.
```{r}
# regression without leverage points
reg.out4 <- lm(log(ViolentCrimesPerPop+1)~., data=df[!(rownames(df) %in% lev),])

se  <- summary(reg.out4)$sigma                  # se
rsq <- summary(reg.out4)$r.squared              # R^2
adrsq <- summary(reg.out4)$adj.r.squared        # adj R^2
cat("RSE:", round(se,2), "\n")
cat("R^2:", round(rsq,2), "\n")
cat("adjusted R^2:", round(adrsq,2))
```

```{r, fig.height=10, fig.width=16}
par(mfrow=c(2,2))
plot(reg.out4)
par(mfrow=c(1,1))
```

The removal of these problematic points does not seem to significantly change the model. As a matter of fact, the RSE remains the same, while the adjusted $R^2$ decreases. Taking everything into account, we decided to keep these rows of the dataset avoiding to lose information.



### Collinearity

Finally, a possible problem, in particular dealing with a large number of variables as in our case, is that of collinearity. It refers to the situation in which two or more predictor variables
are closely related to one another. The presence of collinearity can pose problems in
the regression context, since it can be difficult to separate out the individual effects of collinear variables on the response.

In addition, since collinearity reduces the accuracy of the estimates of the regression
coefficients, it causes the standard error for $\hat \beta_j$ to grow. Recall that the
t-statistic for each predictor is calculated by dividing $\hat \beta_j$ by its standard
error. Consequently, collinearity results in a decline in the t-statistic. As a
result, in the presence of collinearity, we may fail to reject $H_0:\ \beta_j = 0$. This
means that the power of the hypothesis test is reduced by collinearity. 

To avoid such a situation, it is desirable to identify and address potential
collinearity problems while fitting the model.
A simple way to detect collinearity is to look at the correlation matrix
of the predictors.

#### Correlation insight

In the following few lines of code we perform a first manual skimming of strongly dependent variables removing, between the most correlated ones, the more redundant and meaningless. It would have been possible to use a simple for loop or an *ad hoc* package, such as *caret*, to filter out some of the most correlated columns to clean up and resize the dataset, but due to the possibly problematic interactions among variables we talked above about, we preferred to follow a more guided path. In the analysis just the numerical predicting variables and the two main target variables are kept, to highlight eventual correlation-related issues.

```{r, include=FALSE}
# install.packages('corrplot')
library(corrplot)
```

```{r}
# restrict to the predictive numeric attributes
coltodrop <- c(1,2, seq(103,118))
corrdf <- cleandf[,-coltodrop]

# correlation matrix
cm <- cor(corrdf, use='complete.obs')

par(mfrow=c(1,2))
# correlation matrix plot
corrplot(cm, method='color', type='lower', tl.pos='l', tl.col='black', tl.cex=0.35) # order=!!!

# correlation thrashold
threshold <- 0.8
# only strongly correlated attributes highlighted
cma <- abs(cm) > threshold
# number of strong correlations
(sum(cma) - dim(corrdf)[2]) / 2

# filtered correlation matrix plot
corrplot(cma, method='color', type='lower', tl.pos='l', tl.col='black', tl.cex=0.35)

par(mfrow=c(1,1))
```

```{r}
# correlation tradeoff
threshold <- 0.55
# only strongly correlated attributes
cma <- abs(cm) > threshold
# strongly correlated attributes to "ViolentCrimesPerPop"
names(cma[101,][cma[101,]])
```

With a threshold of 0.8 no response variable is significanly correlated to any predictor, while at 0.7 "PctKids2Par" and "PctKidsBornNeverMar" appear to be correlated to "ViolentCrimesPerPop". Only when we lower it to 0.55, due to the mutual correlation between the previously mantioned predictors and the new ones, socio-cultural factors show up, highlighting a probable confounding effect hidden between the variables. It is in fact kindly that, in case of non-American-born people, due to law differences between states and logistic problems, a huge number of such families results in analogous conditions to the ones actually without at least one parent, which is a leading condition among the ones positively correlated to the response.

With this in mind, thanks to the following few lines of code, we handle the more correlated variables, trying, at the same time, to preserve as many columns as possible to preserve the original structure of our dataset.

When faced with the problem of collinearity, there are two simple solutions. The first is to drop one of the problematic variables from the regression. This can usually be done without much compromise to the regression fit, since the presence of collinearity implies that the information is redundant. The second solution is to combine the collinear variables together into a single predictor.
We decided to proceed in the first way, thus eliminating the redundant columns resulting from the study above, in particular considering variables with correlarion above 0.8.
Some relevant examples of columns dropped in the resulting process are:

* "population", due to the presence of both absolute and percent measurement of many variables;
* "agePct16t24" and other redundant columns computed on overlapping time periods;
* "OwnOccHiQuart" and other statistics strictly related to other ones (such as the median).

```{r}
# columns with correlation with more meaningful ones higher than threshold in absolute value

threshold <- 0.8

rem9 <- c( 
"population","agePct16t24","numbUrban","pctWSocSec","medFamInc","perCapInc",
"NumUnderPov","PctLess9thGrade","PctOccupMgmtProf","MalePctDivorce","FemalePctDiv","PctFam2Par",
"PctKids2Par" ,"NumKidsBornNeverMar", "PctImmigRec5","PctImmigRec10",      
"PctRecImmig5","PctRecImmig8","PctRecImmig10","PctSpeakEnglOnly" ,  
"PctLargHouseOccup","PersPerOccupHous","PctHousOwnOcc","OwnOccLowQuart",     
"OwnOccHiQuart","RentLowQ","RentHighQ","MedRent",            
"NumInShelters","NumStreet","PctForeignBorn")

rem8 <- c(rem9,
"householdsize","racePctWhite","agePct12t29","medIncome",
"pctWWage","pctWPubAsst","PctPopUnderPov","PersPerFam",         
"PctYoungKids2Par","PctWorkMom","PctKidsBornNeverMar", "PctImmigRec8",       
"PctNotSpeakEnglWell", "PctPersDenseHous","OwnOccMedVal","RentMedian",         
"PctSameCity85")

rem7 <- c(rem8,
"pctWInvInc","PctNotHSGrad","PctBSorMore","PctLargHouseFam","PctHousLess3BR", 
"MedNumBR","PctSameState85" )


corrdf_ind <- corrdf[,!(colnames(corrdf) %in% rem8)]

# final correlation matrix and filter
cm_ind <- cor(corrdf_ind, use='complete.obs')
cma_ind <- abs(cm_ind) > threshold

par(mfrow=c(1,2))
# resulting filtered correlation matrix plot
corrplot(cma_ind, method='color', type='lower', tl.pos='l', tl.col='black', tl.cex=0.35)

# final correlation matrix plot
corrplot(cm_ind, method='color', type='lower', tl.pos='l', tl.col='black', tl.cex=0.35)

par(mfrow=c(1,1))
```

Let's now fit a linear model on the new obtained dataframe to see how it performs.

```{r}
corrdf <- df[,!(colnames(df) %in% rem8)]

reg.out5 <- lm(log(ViolentCrimesPerPop+1)~., data=corrdf)
#summary(reg.out5)

se  <- summary(reg.out5)$sigma                  # se
rsq <- summary(reg.out5)$r.squared              # R^2
adrsq <- summary(reg.out5)$adj.r.squared        # adj R^2
cat("RSE:", round(se,2), "\n")
cat("R^2:", round(rsq,2), "\n")
cat("adjusted R^2:", round(adrsq,2))
```

```{r, fig.height=10, fig.width=16}
par(mfrow=c(2,2))
plot(reg.out5)
par(mfrow=c(1,1))
```

It is possible to notice that the model fitted from the remained 52 predictors performs better than the model obtained with all the 100 ones: the RSE decreases from 0.59 to 0.58 while the adjusted $R^2$ increases from 0.65 to 0.66. This demonstrates the importance of careful analysis of the collinearity problem, eliminating redundant information not only simplifies the model, but makes it more robust.

From this point on, only the 52 variables selected in this section will be considered for the rest of the study.

```{r, include=FALSE}
# remove no more useful variables
rm(list=setdiff(ls(), c("cleandf","crimedata","standf","corrdf","Col")))
```

```{r, eval=FALSE}
# save corrdf to CSV 
write.csv(corrdf, "../data/crimedata-corr.csv", row.names=TRUE)
```

\newpage

# References

[1] "An Introduction to Statistical Learning", G. James, D. Witten, T. Hastie and R. Tibshirani, Springer, 2013.

[2] "DEA History Book, 1876â€“1990" (drug usage & enforcement), US Department of Justice, 1991, USDoJ.gov, webpage: DoJ-DEA-History-1985-1990.

[3] "Guns and Violence: The Enduring Impact of Crack Cocaine Markets on Young Black Males", W.N. Evans, G. Garthwaite, T. Moore, 2018.

[4] "Measuring Crack Cocaine and Its Impact", Fryer, Roland. Harvard University Society of Fellows: 3, 66. Retrieved January 4, 2016.

[5] "The New Jim Crow: Mass Incarceration in the Age of Colorblindness", M. Alexander.

[6] http://www.disastercenter.com/crime/uscrime.htm

[7] https://archive.ics.uci.edu/ml/datasets/Communities+and+Crime+Unnormalized

[8] https://online.stat.psu.edu/stat462/node/171/.

[9] https://peltarion.com/knowledge-center/documentation/modeling-view/build-an-ai-model/loss-functions/mean-squared-logarithmic-error-(msle)

[10] https://towardsdatascience.com/the-mathematics-behind-principal-component-analysis-fff2d7f4b643
